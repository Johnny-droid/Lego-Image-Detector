{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VC Project 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import ast\n",
    "np.random.seed(42)\n",
    "from sklearn.cluster import KMeans\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from mnist import MNIST\n",
    "import warnings\n",
    "from IPython.display import display, Image\n",
    "import os\n",
    "import json\n",
    "import sys\n",
    "import torch\n",
    "from torch import nn\n",
    "from tqdm import tqdm\n",
    "from torchsummary import summary\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms, models\n",
    "import torchvision.transforms as transforms\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "\n",
    "# ======================== Suppress Warnings ========================\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_file_names(directory):\n",
    "    file_names = []\n",
    "    for filename in os.listdir(directory):\n",
    "        if os.path.isfile(os.path.join(directory, filename)):\n",
    "            file_names.append(filename)\n",
    "    return file_names\n",
    "\n",
    "def loadImage(num, imgLoadSizeRatio = 1, dataDir = 'data/imgs', standardSize = -1):\n",
    "    img = cv2.imread(os.path.join(dataDir, f'{num}'))\n",
    "    if standardSize > 0:\n",
    "        img = cv2.resize(img, (standardSize, standardSize))\n",
    "    elif imgLoadSizeRatio != 1:\n",
    "        img = cv2.resize(img, (0, 0), fx = imgLoadSizeRatio, fy = imgLoadSizeRatio)\n",
    "    return img\n",
    "\n",
    "def render(image):\n",
    "    if image.dtype == np.float64:\n",
    "        image = cv2.convertScaleAbs(image)\n",
    "    if len(image.shape) == 3 and image.shape[2] == 3: # BGR or RGB\n",
    "        if np.array_equal(image[:, :, 0], image[:, :, 2]):\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    img_bytes = cv2.imencode('.png', image)[1].tobytes()\n",
    "    display(Image(data=img_bytes))\n",
    "\n",
    "df_lego_sets = pd.read_csv(\"data/values.csv\")\n",
    "\n",
    "def getActualPieceCount(imgID):\n",
    "    global df_lego_sets\n",
    "    piece_count = df_lego_sets.loc[df_lego_sets['id'] == imgID, 'lego_count'].values[0]\n",
    "    return piece_count \n",
    "\n",
    "def getBoundingBoxes(name):\n",
    "    global df_lego_sets\n",
    "    bb_list_str = df_lego_sets.loc[df_lego_sets['id'] == name, 'bb_list'].values[0]\n",
    "    bb_list = ast.literal_eval(bb_list_str)\n",
    "    return bb_list\n",
    "\n",
    "def makeGuess(image_id, num_guess):\n",
    "    piece_count = getActualPieceCount(image_id)\n",
    "    num_legos_error = abs(num_guess - piece_count)\n",
    "    \n",
    "    if(num_legos_error > 0):\n",
    "        print(f\"Error in Lego Count - Guessed: {num_guess} | Actual: {piece_count} legos\")\n",
    "    else :\n",
    "        print(f\"Perfect ({num_guess}) Guess!\")\n",
    "        \n",
    "    return piece_count, num_legos_error\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4672\n",
      "1169\n"
     ]
    }
   ],
   "source": [
    "imgs = []\n",
    "\n",
    "img_names = get_file_names('data/imgs')\n",
    "\n",
    "for name in img_names:\n",
    "    img = loadImage(name)\n",
    "    count = getActualPieceCount(name[:-4])\n",
    "    bbs = getBoundingBoxes(name[:-4])\n",
    "    imgs.append((img, count, bbs))\n",
    "\n",
    "    # apply multiple transformations to the image to upsample the dataset\n",
    "    \n",
    "    \"\"\" # Rotation\n",
    "    rotated_image_90 = cv2.rotate(img, cv2.ROTATE_90_CLOCKWISE)\n",
    "    imgs.append((rotated_image_90, count))\n",
    "    \n",
    "    rotated_image_180 = cv2.rotate(img, cv2.ROTATE_180)\n",
    "    imgs.append((rotated_image_180, count))\n",
    "    \n",
    "    rotated_image_270 = cv2.rotate(img, cv2.ROTATE_90_COUNTERCLOCKWISE)\n",
    "    imgs.append((rotated_image_270, count))\n",
    "    \n",
    "    # Flipping\n",
    "    flipped_image = cv2.flip(img, flipCode=1)\n",
    "    imgs.append((flipped_image, count))\n",
    "    \n",
    "    flipped_image = cv2.flip(img, flipCode=0)\n",
    "    imgs.append((flipped_image, count))\n",
    "    \n",
    "    # Rotation + Flipping\n",
    "    rotated_flipped_image = cv2.flip(rotated_image_90, flipCode=1)\n",
    "    imgs.append((rotated_flipped_image, count)) \"\"\"\n",
    "\n",
    "# suffle the images\n",
    "np.random.shuffle(imgs)    \n",
    "    \n",
    "# pick the first 80% of the images for training\n",
    "train_imgs = imgs[:int(len(imgs)*0.8)]\n",
    "\n",
    "# pick the remaining 20% of the images for validation\n",
    "val_imgs = imgs[int(len(imgs)*0.8):]\n",
    "\n",
    "print(len(train_imgs))\n",
    "print(len(val_imgs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "  def __init__(self, images, labels, transform=None):\n",
    "        self.images = images\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "  \n",
    "  def __len__(self):\n",
    "    return len(self.images)\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "      image = self.images[idx]\n",
    "\n",
    "      # Apply transformations to the image\n",
    "      if self.transform:\n",
    "        image = self.transform(image)\n",
    "\n",
    "      label = int(self.labels[idx])\n",
    "      return (image, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "146\n",
      "37\n",
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "train_images = [img[0] for img in train_imgs]\n",
    "train_labels = [img[1] for img in train_imgs]\n",
    "val_images = [img[0] for img in val_imgs]\n",
    "val_labels = [img[1] for img in val_imgs]\n",
    "\n",
    "batch_size = 32 # how many images are processed at a time\n",
    "num_workers = 4 # how many processes are used to load the data\n",
    "\n",
    "# Define transformations\n",
    "data_aug = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "# Create dataset objects\n",
    "training_data = CustomDataset(train_images, train_labels, transform=data_aug)\n",
    "validation_data = CustomDataset(val_images, val_labels, transform=data_aug)\n",
    "\n",
    "# Define data loaders\n",
    "train_dataloader = DataLoader(training_data, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
    "validation_dataloader = DataLoader(validation_data, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
    "\n",
    "# Show one image\n",
    "# plt.imshow(training_data[0][0][0,:,:], cmap='gray')\n",
    "\n",
    "print(len(train_dataloader))\n",
    "print(len(validation_dataloader))\n",
    "\n",
    "# Get cpu or gpu device for training\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ConvolutionalNeuralNetwork(\n",
      "  (layers): Sequential(\n",
      "    (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (3): ReLU()\n",
      "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (5): Dropout(p=0.25, inplace=False)\n",
      "    (6): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (7): ReLU()\n",
      "    (8): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (9): ReLU()\n",
      "    (10): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (11): Dropout(p=0.25, inplace=False)\n",
      "    (12): Flatten(start_dim=1, end_dim=-1)\n",
      "    (13): Linear(in_features=179776, out_features=512, bias=True)\n",
      "    (14): ReLU()\n",
      "    (15): Dropout(p=0.5, inplace=False)\n",
      "    (16): Linear(in_features=512, out_features=33, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class ConvolutionalNeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvolutionalNeuralNetwork, self).__init__()\n",
    "        self.pool_size = 2\n",
    "        self.nb_filters = 32\n",
    "        self.kernel_size = 3\n",
    "\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Conv2d(3, self.nb_filters, self.kernel_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(self.nb_filters, self.nb_filters, self.kernel_size),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(self.pool_size),\n",
    "            nn.Dropout(0.25),\n",
    "            nn.Conv2d(self.nb_filters, self.nb_filters * 2, self.kernel_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(self.nb_filters * 2, self.nb_filters * 2, self.kernel_size),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(self.pool_size),\n",
    "            nn.Dropout(0.25),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(179776, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(512, 33)  # we have 33 classes to guess\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        logits = self.layers(x)\n",
    "        return logits\n",
    "\n",
    "model = ConvolutionalNeuralNetwork().to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create folder to save the models\n",
    "try:\n",
    "    folder_name = \"models\"\n",
    "    os.mkdir(folder_name)\n",
    "except FileExistsError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epoch_iter(dataloader, model, loss_fn, optimizer=None, is_train=True):\n",
    "    if is_train:\n",
    "      assert optimizer is not None, \"When training, please provide an optimizer.\"\n",
    "\n",
    "    num_batches = len(dataloader)\n",
    "    \n",
    "    if num_batches == 0:\n",
    "      print(\"No data in the dataloader\")\n",
    "      return 0.0, 0.0\n",
    "\n",
    "    if is_train:\n",
    "      model.train()\n",
    "    else:\n",
    "      model.eval()\n",
    "\n",
    "    total_loss = 0.0\n",
    "    preds = []\n",
    "    labels = []\n",
    "\n",
    "    with torch.set_grad_enabled(is_train):\n",
    "      for batch, (X, y) in enumerate(tqdm(dataloader)):\n",
    "          X, y = X.to(device), y.to(device)\n",
    "\n",
    "          # Obtain prediction\n",
    "          pred = model(X)\n",
    "          \n",
    "          # Obtain loss value\n",
    "          loss = loss_fn(pred, y)\n",
    "\n",
    "          if is_train:\n",
    "            # Backpropagation\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "          # Save training metrics\n",
    "          total_loss += loss.item() # IMPORTANT: call .item() to obtain the value of the loss WITHOUT the computational graph attached\n",
    "\n",
    "          # Calculate final prediction\n",
    "          probs = F.softmax(pred, dim=1)\n",
    "          final_pred = torch.argmax(probs, dim=1)\n",
    "          preds.extend(final_pred.cpu().numpy())\n",
    "          labels.extend(y.cpu().numpy())\n",
    "\n",
    "    return total_loss / num_batches, accuracy_score(labels, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, model_name, num_epochs, train_dataloader, validation_dataloader, loss_fn, optimizer):\n",
    "  \n",
    "  train_history = {'loss': [], 'accuracy': []}\n",
    "  val_history = {'loss': [], 'accuracy': []}\n",
    "  \n",
    "  best_val_loss = np.inf\n",
    "  \n",
    "  print(\"Start training...\")\n",
    "  \n",
    "  for t in range(num_epochs):\n",
    "    \n",
    "      print(f\"\\nEpoch {t+1}\")\n",
    "      \n",
    "      train_loss, train_acc = epoch_iter(train_dataloader, model, loss_fn, optimizer)\n",
    "      print(f\"Train loss: {train_loss:.3f} \\t Train acc: {train_acc:.3f}\")\n",
    "      \n",
    "      if(train_acc > 0):\n",
    "        val_loss, val_acc = epoch_iter(validation_dataloader, model, loss_fn, is_train=False)\n",
    "      else :\n",
    "        val_loss = 0\n",
    "        val_acc = 0\n",
    "        \n",
    "      print(f\"Val loss: {val_loss:.3f} \\t Val acc: {val_acc:.3f}\")\n",
    "\n",
    "      # save model when val loss improves\n",
    "      if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        save_dict = {'model': model.state_dict(), 'optimizer': optimizer.state_dict(), 'epoch': t}\n",
    "        torch.save(save_dict, \"models/\" + model_name + '_best_model.pth')\n",
    "\n",
    "      # save latest model\n",
    "      save_dict = {'model': model.state_dict(), 'optimizer': optimizer.state_dict(), 'epoch': t}\n",
    "      torch.save(save_dict, \"models/\" + model_name + '_latest_model.pth')\n",
    "\n",
    "      # save training history for plotting purposes\n",
    "      train_history[\"loss\"].append(train_loss)\n",
    "      train_history[\"accuracy\"].append(train_acc)\n",
    "\n",
    "      val_history[\"loss\"].append(val_loss)\n",
    "      val_history[\"accuracy\"].append(val_acc)\n",
    "      \n",
    "  print(\"Finished\")\n",
    "  return train_history, val_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotTrainingHistory(train_history, val_history):\n",
    "    plt.subplot(2, 1, 1)\n",
    "    plt.title('Cross Entropy Loss')\n",
    "    plt.plot(train_history['loss'], label='train')\n",
    "    plt.plot(val_history['loss'], label='val')\n",
    "    plt.legend(loc='best')\n",
    "\n",
    "    plt.subplot(2, 1, 2)\n",
    "    plt.title('Classification Accuracy')\n",
    "    plt.plot(train_history['accuracy'], label='train')\n",
    "    plt.plot(val_history['accuracy'], label='val')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.legend(loc='best')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training...\n",
      "\n",
      "Epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 146/146 [06:09<00:00,  2.53s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.938 \t Train acc: 0.868\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 37/37 [00:30<00:00,  1.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 0.498 \t Val acc: 0.918\n",
      "Finished\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAagAAAEYCAYAAAAJeGK1AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAAkS0lEQVR4nO3deZgV9Z3v8fdHaG0QCNAsQjfaJBoXjMHYMiQ6N86YZBAXmNGbQcXJquM13nHJIjdmbsgkk6DOZBJHfXjUMOPC6PWixmVQoxmReMWlMSibBkSQBtEGZTMQQb/3j6omh+Z092lO9znV3Z/X89TDOVW/qvrWr9v++KuqU0cRgZmZWdYcUO4CzMzM8nFAmZlZJjmgzMwskxxQZmaWSQ4oMzPLJAeUmZllkgPKzMwyyQFlXYqk8yTVS9ou6U1Jj0g6uYz1rJa0I62nabqhwHXnSfp6Z9dYCElflvR0ueswy9W73AWYFUrSlcA04GLgMeB9YAIwCdjnj6uk3hGxuwSlnRkRT3T0RktYv1kmeQRlXYKkjwD/AHwjIu6LiPciYldEPBQR307bTJc0R9KdkrYCX5Y0UtKDkt6RtFLShTnbHJeOxrZKekvST9P5lek2NknaLOkFScP3o+YvS3pa0j9JelfS65JOS5f9I/CnwA25oy5JIekbklYAK9J5F6a1v5Mey8icfYSkv5O0StJGSddJOkDSQWn7T+S0HZaO9oa28zg+k/bBlvTfzzQ7xlWStqXHd346/3BJT6XrbJT0f9rbf2ZEhCdPmZ9IRkq7gd6ttJkO7AImk/zPVx/gKeAmoBIYCzQCp6btFwAXpK/7AePT138LPAT0BXoBJwADWtjnauBzLSz7clrPhel2/gewHlC6fB7w9WbrBPA4MDit/8+BjcCngIOAfwXmN2v/ZNr+UOB3TdtMj/uanLaXAQ+1UuvTeeYPBt4FLiA543Ju+r4KOBjYChyZth0BjElf3wVcnf4cKoGTy/075KnrTR5BWVdRBWyMtk95LYiIX0bEh8AQ4GTgqojYGRGLgFtJ/thCEh6HSxoSEdsj4tmc+VXA4RHxQUQsjIitrezzl+lIq2m6MGfZmoi4JSI+AG4j+SPe1mjsJxHxTkTsAM4HZkXEixHxB+B/AZ+WVJvT/pq0/RvAz0hChHR/50lq+u/8AuCONvbd3OnAioi4IyJ2R8RdwCvAmenyD4FjJfWJiDcjYmk6fxdwGDAy7Xtf37J2c0BZV7EJGCKpreuma3NejwTeiYhtOfPWANXp668BHwdeSU9dnZHOv4PkGtfdktZLulZSRSv7nBwRA3OmW3KWbWh6ERG/T1/2a+cxrMnZxnaSvqhuof2adB0i4jngPeCzko4CDgcebGPfze21/5x9VEfEe8Bfk1wTfFPSf6b7AfgOIOB5SUslfbWd+zVzQFmXsQDYSXL6rjW5j+dfDwyW1D9n3qHAOoCIWBER5wLDgGuAOZIOjuTa1g8i4hjgM8AZwN90zGG0WGtL89eTjEQAkHQwyehuXU6bUTmvD03XaXIbMJVk9DQnIna2s8a99p+zj6Y+fCwiPk8yMnwFuCWdvyEiLoyIkSSnTG+SdHg79209nAPKuoSI2AL8b+BGSZMl9ZVUIek0Sde2sM5a4BngJ+mND8eRjJpmA0iaKmloejpwc7raB5L+TNInJPUiucayC/igEw7rLeCjbbT5D+ArksZKOgj4MfBcRKzOafNtSYMkjSK5zpR7Q8IdwF+ShNTtbexLaT/tmYC5wMeV3N7fW9JfA8cAD0saLumsNDT/AGwn7SdJ/11STbrdd0lCtzP60LoxB5R1GRHxU+BK4HskNzusBS4FftnKaucCtSQjgfuB70fE4+myCcBSSduBnwNT0hHGIcAcknBaTnKjxZ2t7OMh7f05qPsLPKSfA+ekd/hdn69BRPwa+HvgXuBN4GPAlGbNHgAWAouA/wR+kbN+A/AiSUD8po16PgPsaDZtIRlBfpPk1OJ3gDMiYiPJ349vkvTtO8BngUvSbZ0IPJf27YPAZRHxehv7N9tL091EZtYFSQrgiIhY2UqbWcD6iPhe6SozK54/qGvWjaV3+/0VcHyZSzFrN5/iM+umJP0QWAJc59Nr1hX5FJ+ZmWWSR1BmZpZJmbwGNWTIkKitrS13GWZmVgILFy7cGBH7PCMykwFVW1tLfX19ucswM7MSkNT8aSWAT/GZmVlGOaDMzCyTHFBmZpZJmbwGZWbWU+zatYuGhgZ27mzvc3y7nsrKSmpqaqioaO3LAf7IAWVmVkYNDQ3079+f2tpaJJW7nE4TEWzatImGhgZGjx5d0Do+xWdmVkY7d+6kqqqqW4cTgCSqqqraNVJ0QJmZlVl3D6cm7T1OB5SZmWWSA8rMrAfbvHkzN910U7vXmzhxIps3b+74gnI4oMzMerCWAuqDD1r/AuS5c+cycODATqoq4bv4zMx6sGnTpvHaa68xduxYKioq6NevHyNGjGDRokUsW7aMyZMns3btWnbu3Mlll13GRRddBPzxkXTbt2/ntNNO4+STT+aZZ56hurqaBx54gD59+hRdmwPKzCwjfvDQUpat39qh2zxm5AC+f+aYFpfPmDGDJUuWsGjRIubNm8fpp5/OkiVL9twKPmvWLAYPHsyOHTs48cQTOfvss6mqqtprGytWrOCuu+7illtu4Ytf/CL33nsvU6dOLbp2B5SZme0xbty4vT6ndP3113P//fcDsHbtWlasWLFPQI0ePZqxY8cCcMIJJ7B69eoOqcUBZWaWEa2NdErl4IMP3vN63rx5PPHEEyxYsIC+fftyyimn5P0c00EHHbTnda9evdixY0eH1OKbJMzMerD+/fuzbdu2vMu2bNnCoEGD6Nu3L6+88grPPvtsSWvzCMrMrAerqqripJNO4thjj6VPnz4MHz58z7IJEyYwc+ZMjjvuOI488kjGjx9f0toUESXdYSHq6urCX1hoZj3B8uXLOfroo8tdRsnkO15JCyOirnlbn+IzM7NMckCZmVkmOaDMzCyTHFBmZpZJDigzM8skB5SZmWWSA8rMzNqlX79+JdmPA8rMzDLJT5IwM+vhrrrqKg477DAuueQSAKZPn44k5s+fz7vvvsuuXbv40Y9+xKRJk0paV1EBJWkC8HOgF3BrRMxotvwjwJ3Aoem+/iki/q2YfZqZdVuPTIMNizt2m4d8Ak6b0WqTKVOmcPnll+8JqHvuuYdHH32UK664ggEDBrBx40bGjx/PWWedhaSOra8V+x1QknoBNwKfBxqAFyQ9GBHLcpp9A1gWEWdKGgq8Kml2RLxfVNVmZtZhjj/+eN5++23Wr19PY2MjgwYNYsSIEVxxxRXMnz+fAw44gHXr1vHWW29xyCGHlKyuYkZQ44CVEbEKQNLdwCQgN6AC6K8kcvsB7wC7i9inmVn31cZIpzOdc845zJkzhw0bNjBlyhRmz55NY2MjCxcupKKigtra2rxftdGZirlJohpYm/O+IZ2X6wbgaGA9sBi4LCI+LGKfZmbWCaZMmcLdd9/NnDlzOOecc9iyZQvDhg2joqKCJ598kjVr1pS8pmICKt+JyOaPRv8LYBEwEhgL3CBpQN6NSRdJqpdU39jYWERZZmbWXmPGjGHbtm1UV1czYsQIzj//fOrr66mrq2P27NkcddRRJa+pmFN8DcConPc1JCOlXF8BZkTynR4rJb0OHAU833xjEXEzcDMkX7dRRF1mZrYfFi/+4w0aQ4YMYcGCBXnbbd++vST1FDOCegE4QtJoSQcCU4AHm7V5AzgVQNJw4EhgVRH7NDOzHmK/R1ARsVvSpcBjJLeZz4qIpZIuTpfPBH4I/LukxSSnBK+KiI0dULeZmXVzRX0OKiLmAnObzZuZ83o98IVi9mFm1t1FREk/X1Qu7f0Gdz/qyMysjCorK9m0aVO7/3h3NRHBpk2bqKysLHgdP+rIzKyMampqaGhooCfcvVxZWUlNTU3B7R1QZmZlVFFRwejRo8tdRib5FJ+ZmWWSA8rMzDLJAWVmZpnkgDIzs0xyQJmZWSY5oMzMLJMcUGZmlkkOKDMzyyQHlJmZZZIDyszMMskBZWZmmeSAMjOzTHJAmZlZJjmgzMwskxxQZmaWSQ4oMzPLJAeUmZllkgPKzMwyyQFlZmaZ5IAyM7NMckCZmVkmOaDMzCyTHFBmZpZJDigzM8skB5SZmWWSA8rMzDLJAWVmZpnkgDIzs0wqKqAkTZD0qqSVkqa10OYUSYskLZX0VDH7MzOznqP3/q4oqRdwI/B5oAF4QdKDEbEsp81A4CZgQkS8IWlYkfWamVkPUcwIahywMiJWRcT7wN3ApGZtzgPui4g3ACLi7SL2Z2ZmPUgxAVUNrM1535DOy/VxYJCkeZIWSvqbljYm6SJJ9ZLqGxsbiyjLzMy6g2ICSnnmRbP3vYETgNOBvwD+XtLH820sIm6OiLqIqBs6dGgRZZmZWXew39egSEZMo3Le1wDr87TZGBHvAe9Jmg98EvhdEfs1M7MeoJgR1AvAEZJGSzoQmAI82KzNA8CfSuotqS/wJ8DyIvZpZmY9xH6PoCJit6RLgceAXsCsiFgq6eJ0+cyIWC7pUeBl4EPg1ohY0hGFm5lZ96aI5peNyq+uri7q6+vLXYaZmZWApIURUdd8vp8kYWZmmeSAMjOzTHJAmZlZJjmgzMwskxxQZmaWSQ4oMzPLJAeUmZllkgPKzMwyyQFlZmaZ5IAyM7NMckCZmVkmOaDMzCyTHFBmZpZJDigzM8skB5SZmWWSA8rMzDLJAWVmZpnkgDIzs0zK5Fe+S2oE1pS7jg40BNhY7iIyxn2yL/fJvtwn+XW3fjksIoY2n5nJgOpuJNVHRF2568gS98m+3Cf7cp/k11P6xaf4zMwskxxQZmaWSQ6o0ri53AVkkPtkX+6TfblP8usR/eJrUGZmlkkeQZmZWSY5oMzMLJMcUB1A0mBJj0takf47qIV2EyS9KmmlpGl5ln9LUkga0vlVd75i+0XSdZJekfSypPslDSxZ8R2sgJ+9JF2fLn9Z0qcKXber2t8+kTRK0pOSlktaKumy0lffOYr5PUmX95L0W0kPl67qThQRnoqcgGuBaenracA1edr0Al4DPgocCLwEHJOzfBTwGMkHlIeU+5iy0C/AF4De6etr8q3fFaa2fvZpm4nAI4CA8cBzha7bFaci+2QE8Kn0dX/gdz29T3KWXwn8B/BwuY+nIyaPoDrGJOC29PVtwOQ8bcYBKyNiVUS8D9ydrtfkX4DvAN3prpWi+iUifhURu9N2zwI1nVtup2nrZ0/6/vZIPAsMlDSiwHW7ov3uk4h4MyJeBIiIbcByoLqUxXeSYn5PkFQDnA7cWsqiO5MDqmMMj4g3AdJ/h+VpUw2szXnfkM5D0lnAuoh4qbMLLbGi+qWZr5L8n2NXVMgxttSm0P7paorpkz0k1QLHA891fIklV2yf/Izkf3I/7KT6Sq53uQvoKiQ9ARySZ9HVhW4iz7yQ1Dfdxhf2t7Zy6qx+abaPq4HdwOz2VZcZbR5jK20KWbcrKqZPkoVSP+Be4PKI2NqBtZXLfveJpDOAtyNioaRTOrqwcnFAFSgiPtfSMklvNZ16SIfbb+dp1kBynalJDbAe+BgwGnhJUtP8FyWNi4gNHXYAnaQT+6VpG18CzgBOjfQkexfU6jG20ebAAtbtiorpEyRVkITT7Ii4rxPrLKVi+uQc4CxJE4FKYICkOyNiaifW2/nKfRGsO0zAdex9M8C1edr0BlaRhFHTBdAxedqtpvvcJFFUvwATgGXA0HIfS5H90ObPnuTaQe7F7+fb83vT1aYi+0TA7cDPyn0cWemTZm1OoZvcJFH2ArrDBFQBvwZWpP8OTuePBObmtJtIcsfRa8DVLWyrOwVUUf0CrCQ5374onWaW+5iK6It9jhG4GLg4fS3gxnT5YqCuPb83XXHa3z4BTiY59fVyzu/GxHIfT7l/T3K20W0Cyo86MjOzTPJdfGZmlkkOKDMzyyQHlJmZZZIDyszMMskBZWZmmeSAMjOzTHJAmZlZJjmgzMwskxxQZmaWSQ4oMzPLJAeUmZllkgPKzMwyyQFlmSVpuqQ7O3H7S5u+3E2Jf5P0rqTnJf2ppFc7YZ+HStouqVdHb9usu3FAWVlJOk9SffpH+01Jj0g6uRT7jogxETEvfXsy8HmgJiLGRcRvIuLIYvchabWkPV/qGBFvRES/iPig2G23sD9JWiVpWWds36yUHFBWNpKuBH4G/BgYDhwK3ARMKkM5hwGrI+K9Muy7I/03YBjwUUknlnLHkvwN3dahHFBWFpI+AvwD8I2IuC8i3ouIXRHxUER8u4V1/q+kDZK2SJovaUzOsomSlknaJmmdpG+l84dIeljSZknvSPqNpAPSZaslfU7S14BbgU+nI7kfSDpFUkPO9kdJuk9So6RNkm5I539M0n+l8zZKmi1pYLrsDpLQfSjd7nck1UqKpj/mkkZKejCtbaWkC3P2OV3SPZJuT49rqaS6Nrr2S8ADwNz0dW7/jZH0eLqvtyR9N53fS9J3Jb2W7mdherx71Zq2nSfp6+nrL0v6f5L+RdI7wPTW+qOlfpR0UFrTJ3LaDZO0Q9LQNo7XujEHlJXLp4FK4P52rPMIcATJCOFFYHbOsl8AfxsR/YFjgf9K538TaACGkozSvkvybax7RMQvSL61dEF6+u37ucvT60UPA2uAWqAauLtpMfATkm8JPhoYBUxPt3sB8AZwZrrda/Mc011pfSOBc4AfSzo1Z/lZ6b4GAg8CN7TUOZL6ptuYnU5TJB2YLusPPAE8mu7rcJJvOQa4EjiX5NtcBwBfBX7f0n6a+ROSrykfBvwjrfRHS/0YEX9Ij3FqznbPBZ6IiMYC67BuyAFl5VIFbIyI3YWuEBGzImJb+gdtOvDJdCQGsAs4RtKAiHg3Il7MmT8COCwdof0m2v810uNI/uB+Ox3p7YyIp9OaVkbE4xHxh/SP6U+BzxayUUmjSK59XZVucxHJSO6CnGZPR8Tc9JrVHcAnW9nkXwF/AH5FEgS9gdPTZWcAGyLin9N9bYuI59JlXwe+FxGvRuKliNhUyDEA6yPiXyNid0TsaKM/WuxH4DbgvKbRbdoHdxRYg3VTDigrl03AkEKvW6SnoWakp6G2AqvTRUPSf88mGQGskfSUpE+n868DVgK/Sm8emLYftY4C1uQL0/RU1N3pacWtwJ05NbVlJPBORGzLmbeGZGTRZEPO698Dla302ZeAe9Kw+ANwH388zTcKeK2F9Vpb1pa1uW/a6I8W+zENy/eAz0o6imSE9+B+1mTdhAPKymUBsBOYXGD780hunvgc8BGSU0SQnFIiIl6IiEkkp5p+CdyTzt8WEd+MiI8CZwJXNjuFVoi1wKEtBMNPSE4ZHhcRA0hOUylneWujtfXA4PT0W5NDgXXtrA9JNcCfA1PT63QbSE73TZQ0JD2Gj7WwekvLmm4Y6Zsz75BmbZofX2v90Vo/QjKKmkoyepoTETtbaGc9hAPKyiIitgD/G7hR0mRJfSVVSDpNUr5rNf1JTl9tIvmD+eOmBZIOlHS+pI9ExC5gK/BBuuwMSYdLUs789t7i/TzwJjBD0sGSKiWdlFPXdmCzpGqg+Q0ebwEfbaEP1gLPAD9Jt3kc8DX2vrZWqAuA3wFHAmPT6eMk17fOJTnld4iky9ObEvpL+pN03VuBH0o6QonjJFWlp+jWkYReL0lfpeWQa9Jaf7TWj5Cc0vtLkpC6fT/6wLoZB5SVTUT8lOQC/feARpL/w76UZATU3O0kp7/WAcuAZ5stvwBYnZ5Wupg/XnA/guTmgO0ko7abcj77VGidH5CMvg4nuemhAfjrdPEPgE8BW4D/JDmtlusnwPeU3EX4rTybP5dkNLie5IaR70fE4+2pL/UlkmPbkDsBM4EvpacRP58exwZgBfBn6bo/JRlx/ookxH8B9EmXXUgSMpuAMSSB2poW+6ONfiQiGkhufgngN+3vAutu1P7rxWZmnUPSLJIbL75X7lqs/PzBOjPLBEm1JHciHl/mUiwjCjrFJ2mCpFfTDxLucxeUpEGS7pf0spLnmB2bzh8l6UlJy5V8yPCyjj4AM+v6JP0QWAJcFxGvl7sey4Y2T/GlH677Hcn56wbgBeDciFiW0+Y6YHtE/CC9RfTGiDhV0ghgRES8mN6ptBCYnLuumZlZPoWMoMYBKyNiVUS8T/KJ7+bPSjuG9FPpEfEKUCtpeES82fSByfQi7XL2/oyHmZlZXoVcg6pm7w/jNZA83iTXSyTnjp+WNI7kwZs1JLfYAnvOLx8PPEcbhgwZErW1tQWUZmZmXd3ChQs3RsQ+z10sJKCUZ17z84IzgJ9LWgQsBn4L7Pm0uKR+wL3A5RGxNe9OpIuAiwAOPfRQ6uvrCyjNzMy6Oklr8s0vJKAaSB5R0qSG5DMbe6Sh85V0RwJeTyckVZCE0+yIaP4Zkdxt3AzcDFBXV+d7383MerhCrkG9ABwhaXT6ZOQpNHtGlqSBTU9NJnnw5PyI2JqG1S+A5emHMs3MzArS5ggqInZLuhR4DOgFzIqIpZIuTpfPJHms/u2SPiD5lP/X0tVPIvmE/+L09B/AdyNibscehpmZdTcFfVA3DZS5zebNzHm9gOSRMs3Xe5r817DMzAzYtWsXDQ0N7NzZ/Z+NW1lZSU1NDRUVFQW195MkzMzKqKGhgf79+1NbW0tyVaR7igg2bdpEQ0MDo0ePLmgdPyzWzKyMdu7cSVVVVbcOJwBJVFVVtWuk6IAyMyuz7h5OTdp7nA4oMzPLJAeUmVkPtnnzZm666aZ2rzdx4kQ2b97c8QXlcECZmfVgLQXUBx+0/sXTc+fOZeDAgZ1UVcJ38ZmZ9WDTpk3jtddeY+zYsVRUVNCvXz9GjBjBokWLWLZsGZMnT2bt2rXs3LmTyy67jIsuugiA2tpa6uvr2b59O6eddhonn3wyzzzzDNXV1TzwwAP06dOnjT23zQFlZpYRP3hoKcvW531c6X47ZuQAvn/mmBaXz5gxgyVLlrBo0SLmzZvH6aefzpIlS/bcCj5r1iwGDx7Mjh07OPHEEzn77LOpqqraaxsrVqzgrrvu4pZbbuGLX/wi9957L1OnTi26dgeUmZntMW7cuL0+p3T99ddz//33A7B27VpWrFixT0CNHj2asWPHAnDCCSewevXqDqnFAWVmlhGtjXRK5eCDD97zet68eTzxxBMsWLCAvn37csopp+T9HNNBBx2053WvXr3YsWNHh9TimyTMzHqw/v37s23btrzLtmzZwqBBg+jbty+vvPIKzz77bElr8wjKzKwHq6qq4qSTTuLYY4+lT58+DB8+fM+yCRMmMHPmTI477jiOPPJIxo8fX9LaFJG9r16qq6sLf2GhmfUEy5cv5+ijjy53GSWT73glLYyIuuZtfYrPzMwyyQFlZmaZ5IAyM7NMckCZmVkmOaDMzCyTHFBmZpZJDigzM2uXfv36lWQ/DigzM8skP0nCzKyHu+qqqzjssMO45JJLAJg+fTqSmD9/Pu+++y67du3iRz/6EZMmTSppXQ4oM7OseGQabFjcsds85BNw2oxWm0yZMoXLL798T0Ddc889PProo1xxxRUMGDCAjRs3Mn78eM466ywkdWx9rXBAmZn1cMcffzxvv/0269evp7GxkUGDBjFixAiuuOIK5s+fzwEHHMC6det46623OOSQQ0pWlwPKzCwr2hjpdKZzzjmHOXPmsGHDBqZMmcLs2bNpbGxk4cKFVFRUUFtbm/erNjqTA8rMzJgyZQoXXnghGzdu5KmnnuKee+5h2LBhVFRU8OSTT7JmzZqS11TQXXySJkh6VdJKSdPyLB8k6X5JL0t6XtKxha5rZmblN2bMGLZt20Z1dTUjRozg/PPPp76+nrq6OmbPns1RRx1V8praHEFJ6gXcCHweaABekPRgRCzLafZdYFFE/KWko9L2pxa4rpmZZcDixX+8QWPIkCEsWLAgb7vt27eXpJ5CRlDjgJURsSoi3gfuBprfa3gM8GuAiHgFqJU0vMB1zczM9lFIQFUDa3PeN6Tzcr0E/BWApHHAYUBNgeuSrneRpHpJ9Y2NjYVVb2Zm3VYhAZXvpvfmX8M7AxgkaRHwP4HfArsLXDeZGXFzRNRFRN3QoUMLKMvMrHvI4jebd4b2Hmchd/E1AKNy3tcA65vtdCvwFQAln+J6PZ36trWumVlPVllZyaZNm6iqqirph2BLLSLYtGkTlZWVBa9TSEC9ABwhaTSwDpgCnJfbQNJA4PfpdaavA/MjYqukNtc1M+vJampqaGhooCdc2qisrKSmpqbg9m0GVETslnQp8BjQC5gVEUslXZwunwkcDdwu6QNgGfC11tZt5zGZmXVbFRUVjB49utxlZJKyeO6zrq4u6uvry12GmZmVgKSFEVHXfL6/bsPMzDLJAWVmZpnkgDIzs0xyQJmZWSY5oMzMLJMcUGZmlkkOKDMzyyQHlJmZZZIDyszMMskBZWZmmeSAMjOzTHJAmZlZJjmgzMwskxxQZmaWSQ4oMzPLJAeUmZllkgPKzMwyyQFlZmaZ5IAyM7NMckCZmVkmOaDMzCyTHFBmZpZJDigzM8skB5SZmWWSA8rMzDLJAWVmZplUUEBJmiDpVUkrJU3Ls/wjkh6S9JKkpZK+krPsinTeEkl3SarsyAMwM7Puqc2AktQLuBE4DTgGOFfSMc2afQNYFhGfBE4B/lnSgZKqgb8D6iLiWKAXMKUD6zczs26qkBHUOGBlRKyKiPeBu4FJzdoE0F+SgH7AO8DudFlvoI+k3kBfYH2HVG5mZt1aIQFVDazNed+Qzst1A3A0SfgsBi6LiA8jYh3wT8AbwJvAloj4Vb6dSLpIUr2k+sbGxnYehpmZdTeFBJTyzItm7/8CWASMBMYCN0gaIGkQyWhrdLrsYElT8+0kIm6OiLqIqBs6dGiB5ZuZWXdVSEA1AKNy3tew72m6rwD3RWIl8DpwFPA54PWIaIyIXcB9wGeKL9vMzLq7QgLqBeAISaMlHUhyk8ODzdq8AZwKIGk4cCSwKp0/XlLf9PrUqcDyjirezMy6r95tNYiI3ZIuBR4juQtvVkQslXRxunwm8EPg3yUtJjkleFVEbAQ2SpoDvEhy08RvgZs751DMzKw7UUTzy0nlV1dXF/X19eUuw8zMSkDSwoioaz7fT5IwM7NMckCZmVkmOaDMzCyTHFBmZpZJDigzM8skB5SZmWWSA8rMzDLJAWVmZpnkgDIzs0xyQJmZWSY5oMzMLJMcUGZmlkkOKDMzyyQHlJmZZZIDyszMMskBZWZmmeSAMjOzTHJAmZlZJjmgzMwskxQR5a5hH5IagTXlrqMDDQE2lruIjHGf7Mt9si/3SX7drV8Oi4ihzWdmMqC6G0n1EVFX7jqyxH2yL/fJvtwn+fWUfvEpPjMzyyQHlJmZZZIDqjRuLncBGeQ+2Zf7ZF/uk/x6RL/4GpSZmWWSR1BmZpZJDigzM8skB1QHkDRY0uOSVqT/Dmqh3QRJr0paKWlanuXfkhSShnR+1Z2v2H6RdJ2kVyS9LOl+SQNLVnwHK+BnL0nXp8tflvSpQtftqva3TySNkvSkpOWSlkq6rPTVd45ifk/S5b0k/VbSw6WruhNFhKciJ+BaYFr6ehpwTZ42vYDXgI8CBwIvAcfkLB8FPEbyAeUh5T6mLPQL8AWgd/r6mnzrd4WprZ992mYi8AggYDzwXKHrdsWpyD4ZAXwqfd0f+F1P75Oc5VcC/wE8XO7j6YjJI6iOMQm4LX19GzA5T5txwMqIWBUR7wN3p+s1+RfgO0B3umulqH6JiF9FxO603bNATeeW22na+tmTvr89Es8CAyWNKHDdrmi/+yQi3oyIFwEiYhuwHKguZfGdpJjfEyTVAKcDt5ay6M7kgOoYwyPiTYD032F52lQDa3PeN6TzkHQWsC4iXursQkusqH5p5qsk/+fYFRVyjC21KbR/uppi+mQPSbXA8cBzHV9iyRXbJz8j+Z/cDzupvpLrXe4CugpJTwCH5Fl0daGbyDMvJPVNt/GF/a2tnDqrX5rt42pgNzC7fdVlRpvH2EqbQtbtiorpk2Sh1A+4F7g8IrZ2YG3lst99IukM4O2IWCjplI4urFwcUAWKiM+1tEzSW02nHtLh9tt5mjWQXGdqUgOsBz4GjAZektQ0/0VJ4yJiQ4cdQCfpxH5p2saXgDOAUyM9yd4FtXqMbbQ5sIB1u6Ji+gRJFSThNDsi7uvEOkupmD45BzhL0kSgEhgg6c6ImNqJ9Xa+cl8E6w4TcB173wxwbZ42vYFVJGHUdAF0TJ52q+k+N0kU1S/ABGAZMLTcx1JkP7T5sye5dpB78fv59vzedLWpyD4RcDvws3IfR1b6pFmbU+gmN0mUvYDuMAFVwK+BFem/g9P5I4G5Oe0mktxx9BpwdQvb6k4BVVS/ACtJzrcvSqeZ5T6mIvpin2MELgYuTl8LuDFdvhioa8/vTVec9rdPgJNJTn29nPO7MbHcx1Pu35OcbXSbgPKjjszMLJN8F5+ZmWWSA8rMzDLJAWVmZpnkgDIzs0xyQJmZWSY5oMzMLJMcUGZmlkn/H0HwOKs9GDWMAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "num_epochs = 1\n",
    "model_name = 'custom_CNN'\n",
    "\n",
    "# Define loss function and optimizer\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer_custom = torch.optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "custom_train_history, custom_val_history = train(model, model_name, num_epochs, train_dataloader, validation_dataloader, loss_fn, optimizer_custom)\n",
    "\n",
    "plotTrainingHistory(custom_train_history, custom_val_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load model\n",
    "model = ConvolutionalNeuralNetwork().to(device)\n",
    "checkpoint = torch.load(\"models/\" + model_name + '_best_model.pth')\n",
    "model.load_state_dict(checkpoint['model'])\n",
    "\n",
    "# Test model\n",
    "# test_loss, test_acc = epoch_iter(test_dataloader, model, loss_fn, is_train=False)\n",
    "# print(f\"\\nTest Loss: {test_loss:.3f} \\nTest Accuracy: {test_acc:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def showErrors(model, dataloader, num_examples=20):    \n",
    "    plt.figure(figsize=(15, 15))\n",
    "\n",
    "    for ind, (X, y) in enumerate(dataloader):\n",
    "      if ind >= 20: break\n",
    "      X, y = X.to(device), y.to(device)    \n",
    "      pred = model(X)\n",
    "      probs = F.softmax(pred, dim=1)\n",
    "      final_pred = torch.argmax(probs, dim=1)\n",
    "\n",
    "      plt.subplot(10, 10, ind + 1)\n",
    "      plt.axis(\"off\")\n",
    "      plt.text(0, -1, y[0].item(), fontsize=14, color='green') # correct\n",
    "      plt.text(8, -1, final_pred[0].item(), fontsize=14, color='red')  # predicted\n",
    "      plt.imshow(X[0][0,:,:].cpu(), cmap='gray')\n",
    "      \n",
    "    plt.show()\n",
    "    \n",
    "# showErrors(model, test_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-trained model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Click here to check the pre-trained models that are available on torchvision.](https://pytorch.org/vision/0.9/models.html)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg = models.vgg16(pretrained=True)\n",
    "vgg.to(device)\n",
    "\n",
    "# set out_features of the last layer to 33\n",
    "vgg.classifier[6] = nn.Linear(4096, 33)\n",
    "\n",
    "print(vgg)\n",
    "\n",
    "# Train network for 10 epochs\n",
    "num_epochs = 1\n",
    "model_name = 'vgg16'\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer_vgg = torch.optim.SGD(vgg.parameters(), lr=0.01)\n",
    "\n",
    "vgg_train_history, vgg_val_history = train(vgg, model_name, num_epochs, train_dataloader, validation_dataloader, loss_fn, optimizer_vgg)\n",
    "\n",
    "plotTrainingHistory(vgg_train_history, vgg_val_history)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
